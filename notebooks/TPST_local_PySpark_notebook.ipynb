{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5c113c",
   "metadata": {},
   "source": [
    "# TPST – Notebook local (PySpark) para ML en siniestros (Perú)\n",
    "\n",
    "Este cuaderno hace **todo** localmente en tu Mac (on‑prem) usando **PySpark** en `local[*]`:\n",
    "\n",
    "1) Arranque de Spark (local).  \n",
    "2) Carga del **Parquet largo** (`year, region, metric, dim_name, dim_value, value`).  \n",
    "3) **Limpieza**: elimina meses y filas con `region = TOTAL`.  \n",
    "4) Persistencia **Silver** (limpio).  \n",
    "5) Construcción **Gold (wide)**: tablón pivoteado `year, region` + columnas por métrica/categoría.  \n",
    "6) **Features**: proporciones, índices (noche/fin de semana) y **lags** por región.  \n",
    "7) **Entrenamiento** (baseline): GLM **Poisson** con Spark ML + métricas (RMSE, MAE).  \n",
    "8) Escritura de salidas `Delta/Parquet` locales.\n",
    "\n",
    "> Requisitos previos (una sola vez):\n",
    "> ```bash\n",
    "> python -m venv .venv && source .venv/bin/activate\n",
    "> python -m pip install -U pip\n",
    "> python -m pip install pyspark==3.5.1 pyarrow==15.0.2 pandas==2.2.2\n",
    "> ```\n",
    "\n",
    "Configura la ruta al Parquet **de entrada** en la celda siguiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuración de rutas ===\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta al Parquet consolidado (tabla larga) generado por tu pipeline\n",
    "PARQUET_IN = Path(\"data/processed/siniestros_normalizado.parquet\")\n",
    "\n",
    "# Carpetas de salida (se crean si no existen)\n",
    "DIR_BRONZE = Path(\"bronze_local\")\n",
    "DIR_SILVER = Path(\"silver_local\")\n",
    "DIR_GOLD   = Path(\"gold_local\")\n",
    "\n",
    "for d in (DIR_BRONZE, DIR_SILVER, DIR_GOLD):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PARQUET_IN.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c96ed",
   "metadata": {},
   "source": [
    "## 1) Inicializar Spark (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"TPST-local\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "         .config(\"spark.driver.memory\", \"4g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34422849",
   "metadata": {},
   "source": [
    "## 2) Cargar Parquet largo → Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bronze = (spark.read.parquet(str(PARQUET_IN))\n",
    "              .select(\"year\",\"region\",\"metric\",\"dim_name\",\"dim_value\",\"value\"))\n",
    "df_bronze.printSchema()\n",
    "df_bronze.show(5, truncate=False)\n",
    "\n",
    "# Persistir una copia Parquet en bronze_local (opcional)\n",
    "(df_bronze\n",
    " .repartition(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(str(DIR_BRONZE / \"siniestros_long_raw.parquet\")))\n",
    "\n",
    "df_bronze.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d071f5",
   "metadata": {},
   "source": [
    "## 3) Limpieza → Silver (quitar meses/TOTAL y tipificar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a17cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MESES = [\"ENERO\",\"FEBRERO\",\"MARZO\",\"ABRIL\",\"MAYO\",\"JUNIO\",\"JULIO\",\"AGOSTO\",\n",
    "         \"SEPTIEMBRE\",\"OCTUBRE\",\"NOVIEMBRE\",\"DICIEMBRE\"]\n",
    "\n",
    "df_silver = (df_bronze\n",
    "    .withColumn(\"region_norm\", F.upper(F.trim(F.col(\"region\"))))\n",
    "    .filter(~F.col(\"region_norm\").isin(MESES))\n",
    "    .filter(~F.col(\"region_norm\").rlike(r'^TOTAL(\\s+NACIONAL|\\s*GENERAL)?$'))\n",
    "    .withColumn(\"year\", F.col(\"year\").cast(\"int\"))\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"double\"))\n",
    "    .drop(\"region\")\n",
    "    .withColumnRenamed(\"region_norm\",\"region\")\n",
    ")\n",
    "\n",
    "df_silver.show(5, truncate=False)\n",
    "df_silver.count()\n",
    "\n",
    "(df_silver\n",
    " .repartition(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(str(DIR_SILVER / \"siniestros_long_clean.parquet\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d8bb1",
   "metadata": {},
   "source": [
    "## 4) Gold (Wide): pivot `year, region` con columnas `metric__categoria`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cfe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata, re\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@F.udf(StringType())\n",
    "def slug(s):\n",
    "    if s is None: \n",
    "        return \"total\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[^A-Za-z0-9]+\",\"_\", s.strip().lower()).strip(\"_\")\n",
    "    return s or \"total\"\n",
    "\n",
    "df_aug = (df_silver\n",
    "  .withColumn(\"dim_value_slug\", slug(F.col(\"dim_value\")))\n",
    "  .withColumn(\"metric_slug\", slug(F.col(\"metric\")))\n",
    "  .withColumn(\"colname\", F.concat_ws(\"__\", F.col(\"metric_slug\"), F.col(\"dim_value_slug\")))\n",
    ")\n",
    "\n",
    "df_wide = (df_aug\n",
    "  .groupBy(\"year\",\"region\")\n",
    "  .pivot(\"colname\")\n",
    "  .agg(F.sum(\"value\"))\n",
    "  .fillna(0.0)\n",
    ")\n",
    "\n",
    "df_wide.printSchema()\n",
    "df_wide.show(5, truncate=False)\n",
    "df_wide.count()\n",
    "\n",
    "(df_wide\n",
    " .repartition(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(str(DIR_GOLD / \"siniestros_wide.parquet\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6009b2",
   "metadata": {},
   "source": [
    "## 5) Features: proporciones, índices e **lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54434e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = df_wide\n",
    "\n",
    "TOTAL_COL = \"siniestros_total__total\"\n",
    "has_total = TOTAL_COL in w.columns\n",
    "\n",
    "franjas = [c for c in w.columns if c.startswith(\"siniestros_por_franja_horaria__\")]\n",
    "dias    = [c for c in w.columns if c.startswith(\"siniestros_por_dia__\")]\n",
    "\n",
    "if has_total:\n",
    "    for c in franjas:\n",
    "        w = w.withColumn(f\"prop__{c}\", F.when(F.col(TOTAL_COL)>0, F.col(c)/F.col(TOTAL_COL)).otherwise(F.lit(None)))\n",
    "    for c in dias:\n",
    "        w = w.withColumn(f\"prop__{c}\", F.when(F.col(TOTAL_COL)>0, F.col(c)/F.col(TOTAL_COL)).otherwise(F.lit(None)))\n",
    "    w = (w\n",
    "         .withColumn(\"idx_noche\", F.col(\"prop__siniestros_por_franja_horaria__18_00_23_59\"))\n",
    "         .withColumn(\"idx_finde\", (F.coalesce(F.col(\"prop__siniestros_por_dia__sabado\"),F.lit(0.0)) +\n",
    "                                   F.coalesce(F.col(\"prop__siniestros_por_dia__domingo\"),F.lit(0.0)))\n",
    "        ))\n",
    "\n",
    "from pyspark.sql import Window\n",
    "win = Window.partitionBy(\"region\").orderBy(\"year\")\n",
    "if has_total:\n",
    "    w = (w\n",
    "        .withColumn(\"y_lag1\", F.lag(F.col(TOTAL_COL), 1).over(win))\n",
    "        .withColumn(\"y_lag2\", F.lag(F.col(TOTAL_COL), 2).over(win))\n",
    "        .withColumn(\"growth_y\", (F.col(TOTAL_COL) - F.col(\"y_lag1\"))/F.col(\"y_lag1\"))\n",
    "    )\n",
    "\n",
    "w.printSchema()\n",
    "w.show(5, truncate=False)\n",
    "\n",
    "(w\n",
    " .repartition(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(str(DIR_GOLD / \"siniestros_wide_features.parquet\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ac44a",
   "metadata": {},
   "source": [
    "## 6) Baseline ML (Spark ML – Poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df_feat = spark.read.parquet(str(DIR_GOLD / \"siniestros_wide_features.parquet\")).na.fill(0.0)\n",
    "\n",
    "target = \"siniestros_total__total\"\n",
    "if target not in df_feat.columns:\n",
    "    raise ValueError(f\"No se encontró la columna objetivo '{target}'. Ajusta el target o revisa si existe la hoja de totales.\")\n",
    "\n",
    "exclude = {\"year\",\"region\", target}\n",
    "feature_cols = [c for c in df_feat.columns if c not in exclude]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(df_feat).select(\"year\",\"region\",\"features\", F.col(target).alias(\"label\"))\n",
    "\n",
    "train = data.filter(\"year <= 2021\")\n",
    "test  = data.filter(\"year >= 2022\")\n",
    "\n",
    "glm = GeneralizedLinearRegression(family=\"poisson\", link=\"log\", maxIter=200, regParam=0.0)\n",
    "model = glm.fit(train)\n",
    "\n",
    "pred = model.transform(test)\n",
    "rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(pred)\n",
    "mae  = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(pred)\n",
    "\n",
    "print(f\"RMSE = {rmse:.2f} | MAE = {mae:.2f}\")\n",
    "pred.select(\"year\",\"region\",\"label\",\"prediction\").orderBy(\"year\",\"region\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f59f7",
   "metadata": {},
   "source": [
    "## 7) Exportaciones adicionales (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a359cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar CSV de las tablas clave para inspección rápida (opcional)\n",
    "(df_silver.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(str(DIR_SILVER / \"siniestros_long_clean_csv\")))\n",
    "(w.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(str(DIR_GOLD / \"siniestros_wide_features_csv\")))\n",
    "\n",
    "# Conteos por año para sanity-check\n",
    "df_silver.groupBy(\"year\").count().orderBy(\"year\").show(50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}