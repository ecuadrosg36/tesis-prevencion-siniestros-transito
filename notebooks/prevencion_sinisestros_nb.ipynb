{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4d9dd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dc508a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnhanced TPST - PySpark ML Pipeline for Traffic Accidents in Peru\\nThis version includes:\\n1. Data recovery for lost regions\\n2. Improved feature engineering\\n3. Multiple model approaches\\n4. Better validation strategies\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced TPST - PySpark ML Pipeline for Traffic Accidents in Peru\n",
    "This version includes:\n",
    "1. Data recovery for lost regions\n",
    "2. Improved feature engineering\n",
    "3. Multiple model approaches\n",
    "4. Better validation strategies\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9925aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "578f6c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 4.0.1\n",
      "Java : 17.0.16\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess, sys\n",
    "\n",
    "# Forzar JAVA_HOME=17 en este proceso\n",
    "java17 = subprocess.check_output([\"/usr/libexec/java_home\", \"-v\", \"17\"]).decode().strip()\n",
    "os.environ[\"JAVA_HOME\"] = java17\n",
    "os.environ[\"PATH\"] = f'{java17}/bin:' + os.environ[\"PATH\"]\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"check-17\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.shuffle.partitions\",\"8\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"Java :\", spark.sparkContext._jvm.java.lang.System.getProperty(\"java.version\"))\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d782247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Input paths - adjust to your environment\n",
    "PARQUET_IN = Path(\"/path/to/your/data/processed/siniestros_normalizado.parquet\")\n",
    "DIR_BRONZE = Path(\"bronze_local\")\n",
    "DIR_SILVER = Path(\"silver_local\")\n",
    "DIR_GOLD = Path(\"gold_local\")\n",
    "DIR_MODELS = Path(\"models\")\n",
    "\n",
    "for d in (DIR_BRONZE, DIR_SILVER, DIR_GOLD, DIR_MODELS):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "TOTAL_COL = \"siniestros_total__total\"\n",
    "MESES = [\"ENERO\",\"FEBRERO\",\"MARZO\",\"ABRIL\",\"MAYO\",\"JUNIO\",\"JULIO\",\"AGOSTO\",\n",
    "         \"SEPTIEMBRE\",\"OCTUBRE\",\"NOVIEMBRE\",\"DICIEMBRE\"]\n",
    "\n",
    "# ============================================\n",
    "# SPARK INITIALIZATION\n",
    "# ============================================\n",
    "\n",
    "def init_spark(app_name=\"TPST-Enhanced\", memory=\"8g\"):\n",
    "    \"\"\"Initialize Spark with optimized settings\"\"\"\n",
    "    return (SparkSession.builder\n",
    "            .appName(app_name)\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "            .config(\"spark.driver.memory\", memory)\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            .getOrCreate())\n",
    "\n",
    "spark = init_spark()\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a724bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "@F.udf(StringType())\n",
    "def slug(s):\n",
    "    \"\"\"Convert string to slug format\"\"\"\n",
    "    if s is None: \n",
    "        return \"total\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[^A-Za-z0-9]+\",\"_\", s.strip().lower()).strip(\"_\")\n",
    "    return s or \"total\"\n",
    "\n",
    "def sum_cols(df, cols):\n",
    "    \"\"\"Safe sum of columns with null handling\"\"\"\n",
    "    if not cols: \n",
    "        return F.lit(None).cast(\"double\")\n",
    "    expr = F.coalesce(F.col(cols[0]), F.lit(0.0))\n",
    "    for c in cols[1:]:\n",
    "        expr = expr + F.coalesce(F.col(c), F.lit(0.0))\n",
    "    return expr\n",
    "\n",
    "def array_sum(cols):\n",
    "    \"\"\"Array-based sum for better performance\"\"\"\n",
    "    return F.aggregate(\n",
    "        F.array(*[F.coalesce(F.col(c), F.lit(0.0)) for c in cols]),\n",
    "        F.lit(0.0),\n",
    "        lambda acc, x: acc + x\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0567407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 1: Loading and creating Bronze layer ===\n",
      "Source file not found, but bronze layer exists at bronze_local/siniestros_long_raw.parquet\n",
      "Using existing bronze layer...\n",
      "Bronze records: 18363\n",
      "root\n",
      " |-- year: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- metric: string (nullable = true)\n",
      " |-- dim_name: string (nullable = true)\n",
      " |-- dim_value: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. DATA LOADING & BRONZE LAYER\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 1: Loading and creating Bronze layer ===\")\n",
    "\n",
    "# Check if we should load from source or use existing bronze\n",
    "use_existing_bronze = False\n",
    "bronze_path = DIR_BRONZE / \"siniestros_long_raw.parquet\"\n",
    "\n",
    "if not PARQUET_IN.exists() and bronze_path.exists():\n",
    "    print(f\"Source file not found, but bronze layer exists at {bronze_path}\")\n",
    "    print(\"Using existing bronze layer...\")\n",
    "    use_existing_bronze = True\n",
    "    df_bronze = spark.read.parquet(str(bronze_path))\n",
    "elif PARQUET_IN.exists():\n",
    "    print(f\"Loading data from: {PARQUET_IN}\")\n",
    "    df_bronze = (spark.read.parquet(str(PARQUET_IN))\n",
    "                 .select(\"year\",\"region\",\"metric\",\"dim_name\",\"dim_value\",\"value\"))\n",
    "    \n",
    "    # Save bronze for future use\n",
    "    (df_bronze\n",
    "     .repartition(4)\n",
    "     .write.mode(\"overwrite\")\n",
    "     .parquet(str(bronze_path)))\n",
    "    print(f\"Bronze layer saved to: {bronze_path}\")\n",
    "else:\n",
    "    # If neither exists, try to load from the silver layer as fallback\n",
    "    silver_path = DIR_SILVER / \"siniestros_long_clean.parquet\"\n",
    "    if silver_path.exists():\n",
    "        print(f\"No source or bronze found, loading from silver layer: {silver_path}\")\n",
    "        df_bronze = spark.read.parquet(str(silver_path))\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cannot find data file. Please ensure one of these exists:\\n\"\n",
    "            f\"  1. Source: {PARQUET_IN}\\n\"\n",
    "            f\"  2. Bronze: {bronze_path}\\n\"\n",
    "            f\"  3. Silver: {silver_path}\"\n",
    "        )\n",
    "\n",
    "print(f\"Bronze records: {df_bronze.count()}\")\n",
    "df_bronze.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d571dfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 2: Creating Silver layer (cleaned) ===\n",
      "Silver records: 17314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 2. SILVER LAYER - Cleaning\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 2: Creating Silver layer (cleaned) ===\")\n",
    "\n",
    "df_silver = (df_bronze\n",
    "    .withColumn(\"region_norm\", F.upper(F.trim(F.col(\"region\"))))\n",
    "    .filter(~F.col(\"region_norm\").isin(MESES))\n",
    "    .filter(~F.col(\"region_norm\").rlike(r'^TOTAL(\\s+NACIONAL|\\s*GENERAL)?$'))\n",
    "    .withColumn(\"year\", F.col(\"year\").cast(\"int\"))\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"double\"))\n",
    "    .drop(\"region\")\n",
    "    .withColumnRenamed(\"region_norm\",\"region\")\n",
    ")\n",
    "\n",
    "print(f\"Silver records: {df_silver.count()}\")\n",
    "\n",
    "(df_silver\n",
    " .repartition(4)\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(str(DIR_SILVER / \"siniestros_long_clean.parquet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c965c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 3: Creating Gold layer (wide format) ===\n",
      "Wide format records: 447\n",
      "Number of columns: 62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 3. GOLD LAYER - Wide format with pivot\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 3: Creating Gold layer (wide format) ===\")\n",
    "\n",
    "df_aug = (df_silver\n",
    "  .withColumn(\"dim_value_slug\", slug(F.col(\"dim_value\")))\n",
    "  .withColumn(\"metric_slug\", slug(F.col(\"metric\")))\n",
    "  .withColumn(\"colname\", F.concat_ws(\"__\", F.col(\"metric_slug\"), F.col(\"dim_value_slug\")))\n",
    ")\n",
    "\n",
    "df_wide = (df_aug\n",
    "  .groupBy(\"year\",\"region\")\n",
    "  .pivot(\"colname\")\n",
    "  .agg(F.sum(\"value\"))\n",
    "  .fillna(0.0)\n",
    ")\n",
    "\n",
    "print(f\"Wide format records: {df_wide.count()}\")\n",
    "print(f\"Number of columns: {len(df_wide.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8e48ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 4: Feature Engineering with Data Recovery ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 4. FEATURE ENGINEERING WITH DATA RECOVERY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 4: Feature Engineering with Data Recovery ===\")\n",
    "\n",
    "# Identify column groups\n",
    "cnt_franja_all = [c for c in df_wide.columns \n",
    "                  if c.startswith(\"siniestros_por_franja_horaria__\") and not c.startswith(\"prop__\")]\n",
    "cnt_dia_all = [c for c in df_wide.columns \n",
    "              if c.startswith(\"siniestros_por_dia__\") and not c.startswith(\"prop__\")]\n",
    "\n",
    "# Separate fine-grained vs broad time bands to avoid double counting\n",
    "is_fine = lambda name: bool(re.search(r\"__\\d{2}_\\d{2}_a_\\d{2}_\\d{2}$\", name)) and (\"_01_a_\" in name)\n",
    "cnt_franja_fine = [c for c in cnt_franja_all if is_fine(c)]\n",
    "cnt_franja_broad = [c for c in cnt_franja_all if c not in cnt_franja_fine]\n",
    "\n",
    "# Use only fine bands if available, otherwise use all\n",
    "cnt_franja = cnt_franja_fine if cnt_franja_fine else cnt_franja_all\n",
    "\n",
    "# Calculate alternative totals for validation\n",
    "df_wide = (df_wide\n",
    "    .withColumn(\"sum_cnt_dia\", sum_cols(df_wide, cnt_dia_all) if cnt_dia_all else F.lit(None))\n",
    "    .withColumn(\"sum_cnt_franja\", sum_cols(df_wide, cnt_franja) if cnt_franja else F.lit(None))\n",
    ")\n",
    "\n",
    "# FIX: Recover lost regions by recalculating totals when proportions are wrong\n",
    "df_wide = df_wide.withColumn(\n",
    "    \"total_check\",\n",
    "    F.when(\n",
    "        (F.col(\"sum_cnt_dia\").isNotNull()) & (F.col(\"sum_cnt_dia\") > 0),\n",
    "        F.col(\"sum_cnt_dia\")\n",
    "    ).when(\n",
    "        (F.col(\"sum_cnt_franja\").isNotNull()) & (F.col(\"sum_cnt_franja\") > 0),\n",
    "        F.col(\"sum_cnt_franja\")\n",
    "    ).otherwise(F.col(TOTAL_COL))\n",
    ")\n",
    "\n",
    "# Apply fix when original total seems incorrect\n",
    "df_wide = df_wide.withColumn(\n",
    "    TOTAL_COL,\n",
    "    F.when(\n",
    "        (F.col(TOTAL_COL) <= 0) | \n",
    "        ((F.col(\"total_check\") > 0) & \n",
    "         (F.abs(F.col(\"total_check\") - F.col(TOTAL_COL)) / F.col(TOTAL_COL) > 0.5)),\n",
    "        F.col(\"total_check\")\n",
    "    ).otherwise(F.col(TOTAL_COL))\n",
    ").drop(\"total_check\", \"sum_cnt_dia\", \"sum_cnt_franja\")\n",
    "\n",
    "# Calculate proportions with the corrected totals\n",
    "for c in cnt_dia_all:\n",
    "    prop_col = f\"prop__{c}\"\n",
    "    df_wide = df_wide.withColumn(\n",
    "        prop_col, \n",
    "        F.when(F.col(TOTAL_COL) > 0, F.col(c) / F.col(TOTAL_COL)).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "for c in cnt_franja:\n",
    "    prop_col = f\"prop__{c}\"\n",
    "    df_wide = df_wide.withColumn(\n",
    "        prop_col,\n",
    "        F.when(F.col(TOTAL_COL) > 0, F.col(c) / F.col(TOTAL_COL)).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "# Identify proportion columns\n",
    "prop_franja = [c for c in df_wide.columns if c.startswith(\"prop__siniestros_por_franja_horaria__\")]\n",
    "prop_dia = [c for c in df_wide.columns if c.startswith(\"prop__siniestros_por_dia__\")]\n",
    "\n",
    "# Normalize proportions to sum to 1\n",
    "if prop_dia:\n",
    "    sum_dia = sum_cols(df_wide, prop_dia)\n",
    "    for c in prop_dia:\n",
    "        df_wide = df_wide.withColumn(\n",
    "            c,\n",
    "            F.when(sum_dia > 0.01, F.col(c) / sum_dia).otherwise(F.col(c))\n",
    "        )\n",
    "\n",
    "if prop_franja:\n",
    "    sum_franja = sum_cols(df_wide, prop_franja)\n",
    "    for c in prop_franja:\n",
    "        df_wide = df_wide.withColumn(\n",
    "            c,\n",
    "            F.when(sum_franja > 0.01, F.col(c) / sum_franja).otherwise(F.col(c))\n",
    "        )\n",
    "\n",
    "# Create index features\n",
    "finde_cols = [c for c in prop_dia if c.endswith(\"__sabado\") or c.endswith(\"__domingo\")]\n",
    "if finde_cols:\n",
    "    df_wide = df_wide.withColumn(\"idx_finde\", sum_cols(df_wide, finde_cols))\n",
    "else:\n",
    "    df_wide = df_wide.withColumn(\"idx_finde\", F.lit(0.0))\n",
    "\n",
    "# Night index (simplified)\n",
    "night_cols = [c for c in prop_franja if any(h in c for h in [\"20_\", \"21_\", \"22_\", \"23_\", \"00_\", \"01_\", \"02_\"])]\n",
    "if night_cols:\n",
    "    df_wide = df_wide.withColumn(\"idx_noche\", sum_cols(df_wide, night_cols[:5]))  # Limit to avoid overlap\n",
    "else:\n",
    "    df_wide = df_wide.withColumn(\"idx_noche\", F.lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2da109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: Adding temporal features ===\n",
      "Features dataset records: 447\n",
      "Regions per year check:\n",
      "+----+---------+\n",
      "|year|n_regions|\n",
      "+----+---------+\n",
      "|2007|        1|\n",
      "|2008|       27|\n",
      "|2009|       28|\n",
      "|2010|       28|\n",
      "|2011|       28|\n",
      "|2012|       28|\n",
      "|2013|       28|\n",
      "|2014|       28|\n",
      "|2015|       28|\n",
      "|2016|       28|\n",
      "|2017|       28|\n",
      "|2018|       28|\n",
      "|2019|       28|\n",
      "|2020|       28|\n",
      "|2021|       28|\n",
      "|2022|       28|\n",
      "|2023|       27|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. TEMPORAL FEATURES & LAGS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 5: Adding temporal features ===\")\n",
    "\n",
    "win = Window.partitionBy(\"region\").orderBy(\"year\")\n",
    "\n",
    "df_features = (df_wide\n",
    "    .withColumn(\"y_lag1\", F.lag(F.col(TOTAL_COL), 1).over(win))\n",
    "    .withColumn(\"y_lag2\", F.lag(F.col(TOTAL_COL), 2).over(win))\n",
    "    .withColumn(\"y_lag3\", F.lag(F.col(TOTAL_COL), 3).over(win))\n",
    "    .withColumn(\"growth_y\", \n",
    "        F.when(F.col(\"y_lag1\") > 0, \n",
    "               (F.col(TOTAL_COL) - F.col(\"y_lag1\")) / F.col(\"y_lag1\"))\n",
    "        .otherwise(0.0))\n",
    "    .withColumn(\"rolling_avg_3y\", \n",
    "        F.avg(TOTAL_COL).over(win.rowsBetween(-3, -1)))\n",
    "    .withColumn(\"rolling_std_3y\", \n",
    "        F.stddev(TOTAL_COL).over(win.rowsBetween(-3, -1)))\n",
    ")\n",
    "\n",
    "# Add seasonality indicators\n",
    "df_features = (df_features\n",
    "    .withColumn(\"is_covid_period\", \n",
    "        F.when(F.col(\"year\").isin([2020, 2021]), 1).otherwise(0))\n",
    "    .withColumn(\"years_since_2008\", F.col(\"year\") - 2008)\n",
    ")\n",
    "\n",
    "# Regional encoding based on historical averages\n",
    "region_stats = df_features.groupBy(\"region\").agg(\n",
    "    F.avg(TOTAL_COL).alias(\"region_avg_total\"),\n",
    "    F.stddev(TOTAL_COL).alias(\"region_std_total\"),\n",
    "    F.min(TOTAL_COL).alias(\"region_min_total\"),\n",
    "    F.max(TOTAL_COL).alias(\"region_max_total\")\n",
    ").fillna(0.0)\n",
    "\n",
    "df_features = df_features.join(region_stats, \"region\", \"left\")\n",
    "\n",
    "# Create region size category\n",
    "df_features = df_features.withColumn(\n",
    "    \"region_size_category\",\n",
    "    F.when(F.col(\"region_avg_total\") > 10000, \"very_large\")\n",
    "    .when(F.col(\"region_avg_total\") > 5000, \"large\")\n",
    "    .when(F.col(\"region_avg_total\") > 2000, \"medium\")\n",
    "    .when(F.col(\"region_avg_total\") > 1000, \"small\")\n",
    "    .otherwise(\"very_small\")\n",
    ")\n",
    "\n",
    "# Handle Lima separately flag\n",
    "df_features = df_features.withColumn(\n",
    "    \"is_lima\", \n",
    "    F.when(F.col(\"region\") == \"LIMA\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(f\"Features dataset records: {df_features.count()}\")\n",
    "print(f\"Regions per year check:\")\n",
    "df_features.groupBy(\"year\").agg(F.countDistinct(\"region\").alias(\"n_regions\")).orderBy(\"year\").show()\n",
    "\n",
    "# Save features\n",
    "(df_features\n",
    " .write.mode(\"overwrite\")\n",
    " .parquet(str(DIR_GOLD / \"siniestros_features_enhanced.parquet\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3443630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6: Preparing data for modeling ===\n",
      "Model dataset records: 405\n",
      "Number of features: 89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 6. DATA PREPARATION FOR MODELING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 6: Preparing data for modeling ===\")\n",
    "\n",
    "# Filter out records with insufficient history\n",
    "df_model = (df_features\n",
    "    .filter(F.col(\"y_lag1\").isNotNull())\n",
    "    .filter(F.col(TOTAL_COL) > 0)\n",
    ")\n",
    "\n",
    "# Impute missing values\n",
    "numeric_cols = [c for c in df_model.columns if df_model.schema[c].dataType in [DoubleType(), IntegerType()]]\n",
    "for col in numeric_cols:\n",
    "    df_model = df_model.fillna({col: 0.0})\n",
    "\n",
    "# Create log-transformed target for better model performance\n",
    "df_model = df_model.withColumn(\"log_total\", F.log1p(F.col(TOTAL_COL)))\n",
    "\n",
    "# Select feature columns\n",
    "exclude_cols = {\"year\", \"region\", TOTAL_COL, \"log_total\", \"region_size_category\"}\n",
    "feature_cols = [c for c in df_model.columns if c not in exclude_cols and not c.startswith(\"region_\")]\n",
    "\n",
    "print(f\"Model dataset records: {df_model.count()}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "862a628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 7: Creating train/test splits ===\n",
      "Train records: 351\n",
      "Test records: 54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 7. TRAIN/TEST SPLIT & FEATURE ASSEMBLY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 7: Creating train/test splits ===\")\n",
    "\n",
    "# Time-based split\n",
    "train_data = df_model.filter(F.col(\"year\") <= 2021)\n",
    "test_data = df_model.filter(F.col(\"year\") >= 2022)\n",
    "\n",
    "print(f\"Train records: {train_data.count()}\")\n",
    "print(f\"Test records: {test_data.count()}\")\n",
    "\n",
    "# Handle categorical features\n",
    "string_indexer = StringIndexer(\n",
    "    inputCol=\"region_size_category\", \n",
    "    outputCol=\"region_size_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Feature assembly\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols + [\"region_size_idx\"], \n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "feature_pipeline = Pipeline(stages=[string_indexer, assembler, scaler])\n",
    "\n",
    "# Fit and transform\n",
    "feature_model = feature_pipeline.fit(train_data)\n",
    "train_prepared = feature_model.transform(train_data)\n",
    "test_prepared = feature_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62ea6d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 8: Training multiple models ===\n",
      "\n",
      "--- Training Poisson GLM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:14 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:15 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:16 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:17 WARN Instrumentation: [1aca123f] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "25/10/05 23:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1474.0 KiB\n",
      "25/10/05 23:23:18 WARN DAGScheduler: Broadcasting large task binary with size 1474.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisson GLM - RMSE: 57338.32, MAE: 12863.29\n",
      "\n",
      "--- Training Random Forest ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:19 WARN DAGScheduler: Broadcasting large task binary with size 1051.8 KiB\n",
      "25/10/05 23:23:19 WARN DAGScheduler: Broadcasting large task binary with size 1153.0 KiB\n",
      "25/10/05 23:23:19 WARN DAGScheduler: Broadcasting large task binary with size 1297.2 KiB\n",
      "25/10/05 23:23:20 WARN DAGScheduler: Broadcasting large task binary with size 1559.7 KiB\n",
      "25/10/05 23:23:20 WARN DAGScheduler: Broadcasting large task binary with size 1926.5 KiB\n",
      "25/10/05 23:23:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/10/05 23:23:20 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - RMSE: 1694.83, MAE: 622.59\n",
      "\n",
      "--- Training Gradient Boosted Trees ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1000.7 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1003.0 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1003.4 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1004.5 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1005.2 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1007.1 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1009.0 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1009.5 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1010.5 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1011.2 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1013.2 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1015.8 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1016.3 KiB\n",
      "25/10/05 23:23:27 WARN DAGScheduler: Broadcasting large task binary with size 1017.3 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1018.1 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1020.3 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1022.5 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1023.0 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1024.0 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1024.8 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1027.0 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1029.4 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1029.8 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1030.8 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1031.6 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1033.5 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1035.2 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1035.7 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1036.7 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1037.4 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1039.1 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1041.1 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1041.6 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1042.1 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1043.4 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1044.1 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1046.2 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1046.6 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1047.6 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1048.4 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1050.7 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1053.3 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1053.8 KiB\n",
      "25/10/05 23:23:28 WARN DAGScheduler: Broadcasting large task binary with size 1054.8 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1055.5 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1056.8 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1058.4 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1058.8 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1059.8 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1060.6 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1062.6 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1065.2 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1065.7 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1066.7 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1067.4 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1068.7 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1070.2 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1070.7 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1071.3 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1072.2 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1073.9 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1076.4 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1076.9 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1077.9 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1078.6 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1080.3 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1082.5 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1082.9 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1083.9 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1084.7 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1087.0 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1089.6 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1090.1 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1091.1 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1091.8 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1093.8 KiB\n",
      "25/10/05 23:23:29 WARN DAGScheduler: Broadcasting large task binary with size 1096.0 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1096.5 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1097.5 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1098.2 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1100.3 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1102.5 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1103.5 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1104.3 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1106.5 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1108.8 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1109.3 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1110.3 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1111.1 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1113.3 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1116.1 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1117.9 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1120.2 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1122.7 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1123.1 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1124.1 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1124.8 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1126.5 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1128.7 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1129.2 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1130.2 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1130.9 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1132.9 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1135.4 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1135.8 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1136.9 KiB\n",
      "25/10/05 23:23:30 WARN DAGScheduler: Broadcasting large task binary with size 1137.6 KiB\n",
      "25/10/05 23:23:31 WARN DAGScheduler: Broadcasting large task binary with size 1139.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT (log target) - RMSE: 6782.24, MAE: 2088.42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 8. MODEL TRAINING - Multiple Approaches\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 8: Training multiple models ===\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 8.1 Baseline: Poisson GLM\n",
    "print(\"\\n--- Training Poisson GLM ---\")\n",
    "glm_poisson = GeneralizedLinearRegression(\n",
    "    family=\"poisson\",\n",
    "    link=\"log\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    labelCol=TOTAL_COL,\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "glm_model = glm_poisson.fit(train_prepared)\n",
    "glm_predictions = glm_model.transform(test_prepared)\n",
    "\n",
    "glm_rmse = RegressionEvaluator(\n",
    "    labelCol=TOTAL_COL, \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ").evaluate(glm_predictions)\n",
    "\n",
    "glm_mae = RegressionEvaluator(\n",
    "    labelCol=TOTAL_COL, \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ").evaluate(glm_predictions)\n",
    "\n",
    "results[\"Poisson_GLM\"] = {\"RMSE\": glm_rmse, \"MAE\": glm_mae}\n",
    "print(f\"Poisson GLM - RMSE: {glm_rmse:.2f}, MAE: {glm_mae:.2f}\")\n",
    "\n",
    "# 8.2 Random Forest\n",
    "print(\"\\n--- Training Random Forest ---\")\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=TOTAL_COL,\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_prepared)\n",
    "rf_predictions = rf_model.transform(test_prepared)\n",
    "\n",
    "rf_rmse = RegressionEvaluator(\n",
    "    labelCol=TOTAL_COL, \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ").evaluate(rf_predictions)\n",
    "\n",
    "rf_mae = RegressionEvaluator(\n",
    "    labelCol=TOTAL_COL, \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ").evaluate(rf_predictions)\n",
    "\n",
    "results[\"Random_Forest\"] = {\"RMSE\": rf_rmse, \"MAE\": rf_mae}\n",
    "print(f\"Random Forest - RMSE: {rf_rmse:.2f}, MAE: {rf_mae:.2f}\")\n",
    "\n",
    "# 8.3 Gradient Boosted Trees (on log-transformed target)\n",
    "print(\"\\n--- Training Gradient Boosted Trees ---\")\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=\"log_total\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=5,\n",
    "    maxIter=50,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_prepared)\n",
    "gbt_predictions = gbt_model.transform(test_prepared)\n",
    "\n",
    "# Transform predictions back from log space\n",
    "gbt_predictions = gbt_predictions.withColumn(\n",
    "    \"prediction_exp\", \n",
    "    F.expm1(F.col(\"prediction\"))\n",
    ")\n",
    "\n",
    "gbt_rmse = RegressionEvaluator(\n",
    "    labelCol=TOTAL_COL, \n",
    "    predictionCol=\"prediction_exp\", \n",
    "    metricName=\"rmse\"\n",
    ").evaluate(gbt_predictions)\n",
    "\n",
    "gbt_mae = RegressionEvaluator(\n",
    "    labelCol=TOTAL_COL, \n",
    "    predictionCol=\"prediction_exp\", \n",
    "    metricName=\"mae\"\n",
    ").evaluate(gbt_predictions)\n",
    "\n",
    "results[\"GBT_LogTarget\"] = {\"RMSE\": gbt_rmse, \"MAE\": gbt_mae}\n",
    "print(f\"GBT (log target) - RMSE: {gbt_rmse:.2f}, MAE: {gbt_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a499cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 9: Training separate model for Lima ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:33 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 32 to 13 (= number of training instances)\n",
      "25/10/05 23:23:35 WARN DAGScheduler: Broadcasting large task binary with size 1096.2 KiB\n",
      "25/10/05 23:23:35 WARN DAGScheduler: Broadcasting large task binary with size 1321.7 KiB\n",
      "25/10/05 23:23:35 WARN DAGScheduler: Broadcasting large task binary with size 1688.4 KiB\n",
      "25/10/05 23:23:35 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/10/05 23:23:35 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/10/05 23:23:36 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "25/10/05 23:23:36 WARN DAGScheduler: Broadcasting large task binary with size 4.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate Lima Model - RMSE: 2102.12, MAE: 467.91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 9. SEPARATE MODEL FOR LIMA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 9: Training separate model for Lima ===\")\n",
    "\n",
    "# Split data\n",
    "train_lima = train_prepared.filter(F.col(\"is_lima\") == 1)\n",
    "train_other = train_prepared.filter(F.col(\"is_lima\") == 0)\n",
    "test_lima = test_prepared.filter(F.col(\"is_lima\") == 1)\n",
    "test_other = test_prepared.filter(F.col(\"is_lima\") == 0)\n",
    "\n",
    "if train_lima.count() > 0 and test_lima.count() > 0:\n",
    "    # Model for Lima\n",
    "    rf_lima = RandomForestRegressor(\n",
    "        labelCol=\"log_total\",\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=50,\n",
    "        maxDepth=8,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    rf_lima_model = rf_lima.fit(train_lima)\n",
    "    lima_predictions = rf_lima_model.transform(test_lima)\n",
    "    lima_predictions = lima_predictions.withColumn(\n",
    "        \"prediction_exp\", \n",
    "        F.expm1(F.col(\"prediction\"))\n",
    "    )\n",
    "    \n",
    "    # Model for other regions\n",
    "    rf_other = RandomForestRegressor(\n",
    "        labelCol=TOTAL_COL,\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=100,\n",
    "        maxDepth=10,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    rf_other_model = rf_other.fit(train_other)\n",
    "    other_predictions = rf_other_model.transform(test_other)\n",
    "    \n",
    "    # Combine predictions\n",
    "    combined_predictions = lima_predictions.select(\n",
    "        \"year\", \"region\", TOTAL_COL, \n",
    "        F.col(\"prediction_exp\").alias(\"prediction\")\n",
    "    ).union(\n",
    "        other_predictions.select(\"year\", \"region\", TOTAL_COL, \"prediction\")\n",
    "    )\n",
    "    \n",
    "    combined_rmse = RegressionEvaluator(\n",
    "        labelCol=TOTAL_COL, \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    ).evaluate(combined_predictions)\n",
    "    \n",
    "    combined_mae = RegressionEvaluator(\n",
    "        labelCol=TOTAL_COL, \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"\n",
    "    ).evaluate(combined_predictions)\n",
    "    \n",
    "    results[\"Separate_Lima_Model\"] = {\"RMSE\": combined_rmse, \"MAE\": combined_mae}\n",
    "    print(f\"Separate Lima Model - RMSE: {combined_rmse:.2f}, MAE: {combined_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e382179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 10: Model Evaluation ===\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "Poisson_GLM               RMSE:   57338.32  MAE:   12863.29\n",
      "Random_Forest             RMSE:    1694.83  MAE:     622.59\n",
      "GBT_LogTarget             RMSE:    6782.24  MAE:    2088.42\n",
      "Separate_Lima_Model       RMSE:    2102.12  MAE:     467.91\n",
      "\n",
      "==================================================\n",
      "TOP 20 FEATURE IMPORTANCES (Random Forest)\n",
      "==================================================\n",
      "y_lag1                                             0.315343\n",
      "rolling_avg_3y                                     0.222959\n",
      "y_lag2                                             0.081930\n",
      "idx_finde                                          0.050012\n",
      "prop__siniestros_por_dia__martes                   0.035224\n",
      "prop__siniestros_por_dia__sabado                   0.034782\n",
      "siniestros_por_causa__imprudencia_del_conductor    0.034405\n",
      "prop__siniestros_por_dia__viernes                  0.032220\n",
      "prop__siniestros_por_dia__jueves                   0.027003\n",
      "y_lag3                                             0.023481\n",
      "siniestros_por_dia__jueves                         0.018810\n",
      "is_lima                                            0.017051\n",
      "siniestros_por_causa__ebriedad_del_condutor        0.011548\n",
      "prop__siniestros_por_dia__miercoles                0.009919\n",
      "prop__siniestros_por_dia__domingo                  0.008835\n",
      "siniestros_por_causa__desacato_de_senal_de_transito 0.008220\n",
      "siniestros_por_causa__exceso_de_carga              0.008160\n",
      "region_size_idx                                    0.007797\n",
      "siniestros_por_causa__falla_de_luces               0.006674\n",
      "siniestros_por_tipo__atropello_y_fuga              0.006188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 10. MODEL EVALUATION & FEATURE IMPORTANCE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 10: Model Evaluation ===\")\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:25} RMSE: {metrics['RMSE']:10.2f}  MAE: {metrics['MAE']:10.2f}\")\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "if hasattr(rf_model, 'featureImportances'):\n",
    "    feature_importance = list(zip(feature_cols + [\"region_size_idx\"], \n",
    "                                  rf_model.featureImportances.toArray()))\n",
    "    feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TOP 20 FEATURE IMPORTANCES (Random Forest)\")\n",
    "    print(\"=\"*50)\n",
    "    for feat, imp in feature_importance[:20]:\n",
    "        print(f\"{feat:50} {imp:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71aa56f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 11: Saving best model and predictions ===\n",
      "\n",
      "Best model: Random_Forest\n",
      "\n",
      "Predictions saved to: models/predictions_best_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 11. SAVE BEST MODEL & PREDICTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 11: Saving best model and predictions ===\")\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = min(results, key=lambda x: results[x][\"RMSE\"])\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Save predictions from best model\n",
    "if \"Forest\" in best_model_name:\n",
    "    best_predictions = rf_predictions\n",
    "elif \"GBT\" in best_model_name:\n",
    "    best_predictions = gbt_predictions.select(\n",
    "        \"year\", \"region\", TOTAL_COL, \n",
    "        F.col(\"prediction_exp\").alias(\"prediction\")\n",
    "    )\n",
    "elif \"Separate\" in best_model_name:\n",
    "    best_predictions = combined_predictions\n",
    "else:\n",
    "    best_predictions = glm_predictions\n",
    "\n",
    "# Add prediction quality metrics\n",
    "best_predictions = best_predictions.withColumn(\n",
    "    \"absolute_error\", \n",
    "    F.abs(F.col(\"prediction\") - F.col(TOTAL_COL))\n",
    ").withColumn(\n",
    "    \"relative_error\",\n",
    "    F.when(F.col(TOTAL_COL) > 0, \n",
    "           F.col(\"absolute_error\") / F.col(TOTAL_COL))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Save predictions\n",
    "(best_predictions\n",
    " .select(\"year\", \"region\", TOTAL_COL, \"prediction\", \"absolute_error\", \"relative_error\")\n",
    " .orderBy(\"year\", \"region\")\n",
    " .coalesce(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(str(DIR_MODELS / \"predictions_best_model\")))\n",
    "\n",
    "print(f\"\\nPredictions saved to: {DIR_MODELS / 'predictions_best_model'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f00cbd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 12: Generating validation report ===\n",
      "\n",
      "==================================================\n",
      "PREDICTION QUALITY BY REGION (Top 10 Worst)\n",
      "==================================================\n",
      "+------------+-------------------+-------------------+------------------+\n",
      "|      region| avg_relative_error| max_relative_error|avg_absolute_error|\n",
      "+------------+-------------------+-------------------+------------------+\n",
      "|HUANCAVELICA|  7.538839815115439|  14.97981937229437|1657.5661567460315|\n",
      "|    AREQUIPA| 0.3787827295116913|0.44580425979915783|1939.8066309523806|\n",
      "|       PIURA|0.27641567223535446|0.44139481060560387|1058.6960634920636|\n",
      "|        LIMA| 0.1777040022359835| 0.1969702913198467|  7688.10554166666|\n",
      "|   CAJAMARCA| 0.1754218029414009| 0.3348927272521909|369.66155357142816|\n",
      "|      LORETO| 0.1559244078032248| 0.1961598693452143| 32.35672738372739|\n",
      "|    AYACUCHO|0.06567110935458155|0.09484685655357825| 27.96516994418525|\n",
      "|    AMAZONAS|0.05710442334463447|0.06789502424599089|29.351673599142117|\n",
      "|      CALLAO|0.05683374558428361|0.10411362463480678| 171.5205827922082|\n",
      "|         ICA|0.05439859950479732|0.09834777674610533|117.17458333333343|\n",
      "+------------+-------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Validation report saved to: models/validation_report.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 12. VALIDATION REPORT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 12: Generating validation report ===\")\n",
    "\n",
    "validation_report = {\n",
    "    \"data_quality\": {\n",
    "        \"total_regions\": df_features.select(\"region\").distinct().count(),\n",
    "        \"years_covered\": df_features.select(\"year\").distinct().count(),\n",
    "        \"records_processed\": df_features.count(),\n",
    "        \"records_in_model\": df_model.count(),\n",
    "        \"train_size\": train_data.count(),\n",
    "        \"test_size\": test_data.count()\n",
    "    },\n",
    "    \"model_performance\": results,\n",
    "    \"best_model\": {\n",
    "        \"name\": best_model_name,\n",
    "        \"rmse\": results[best_model_name][\"RMSE\"],\n",
    "        \"mae\": results[best_model_name][\"MAE\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check prediction quality by region\n",
    "pred_quality = best_predictions.groupBy(\"region\").agg(\n",
    "    F.avg(\"relative_error\").alias(\"avg_relative_error\"),\n",
    "    F.max(\"relative_error\").alias(\"max_relative_error\"),\n",
    "    F.avg(\"absolute_error\").alias(\"avg_absolute_error\")\n",
    ").orderBy(F.desc(\"avg_relative_error\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION QUALITY BY REGION (Top 10 Worst)\")\n",
    "print(\"=\"*50)\n",
    "pred_quality.show(10)\n",
    "\n",
    "# Save validation report\n",
    "with open(DIR_MODELS / \"validation_report.json\", \"w\") as f:\n",
    "    json.dump(validation_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nValidation report saved to: {DIR_MODELS / 'validation_report.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22d1b587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 13: Generating future predictions (2024-2025) ===\n",
      "\n",
      "==================================================\n",
      "FUTURE PREDICTIONS (2024-2025) - Sample\n",
      "==================================================\n",
      "+----+--------+-------------------+\n",
      "|year|  region|predicted_accidents|\n",
      "+----+--------+-------------------+\n",
      "|2024|AREQUIPA|  7509.507325396826|\n",
      "|2024|   CUSCO| 3157.5031944444445|\n",
      "|2024|    LIMA|   36132.3217420635|\n",
      "|2024|   PIURA|  3752.115476190476|\n",
      "|2025|AREQUIPA|  7509.507325396826|\n",
      "|2025|   CUSCO| 3157.5031944444445|\n",
      "|2025|    LIMA|   36132.3217420635|\n",
      "|2025|   PIURA|  3752.115476190476|\n",
      "+----+--------+-------------------+\n",
      "\n",
      "\n",
      "Future predictions saved to: models/future_predictions_2024_2025\n",
      "\n",
      "==================================================\n",
      "FUTURE PREDICTIONS (2024-2025) - Sample\n",
      "==================================================\n",
      "+----+--------+-------------------+\n",
      "|year|  region|predicted_accidents|\n",
      "+----+--------+-------------------+\n",
      "|2024|AREQUIPA|  7509.507325396826|\n",
      "|2024|   CUSCO| 3157.5031944444445|\n",
      "|2024|    LIMA|   36132.3217420635|\n",
      "|2024|   PIURA|  3752.115476190476|\n",
      "|2025|AREQUIPA|  7509.507325396826|\n",
      "|2025|   CUSCO| 3157.5031944444445|\n",
      "|2025|    LIMA|   36132.3217420635|\n",
      "|2025|   PIURA|  3752.115476190476|\n",
      "+----+--------+-------------------+\n",
      "\n",
      "\n",
      "Future predictions saved to: models/future_predictions_2024_2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 13. FUTURE PREDICTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 13: Generating future predictions (2024-2025) ===\")\n",
    "\n",
    "# Create future data template\n",
    "regions = df_features.select(\"region\").distinct().collect()\n",
    "future_years = [2024, 2025]\n",
    "future_data = spark.createDataFrame(\n",
    "    [(y, r.region) for y in future_years for r in regions],\n",
    "    [\"year\", \"region\"]\n",
    ")\n",
    "\n",
    "# Get last known values for each region - INCLUDING ALL REQUIRED COLUMNS\n",
    "# First, get the most recent year data for each region\n",
    "last_year_data = df_features.groupBy(\"region\").agg(F.max(\"year\").alias(\"max_year\"))\n",
    "\n",
    "# Join to get all features from the most recent year\n",
    "last_known = df_features.alias(\"df\").join(\n",
    "    last_year_data.alias(\"ly\"),\n",
    "    (F.col(\"df.region\") == F.col(\"ly.region\")) & (F.col(\"df.year\") == F.col(\"ly.max_year\"))\n",
    ").select(\"df.*\")\n",
    "\n",
    "# For future predictions, we'll use the last known values as a baseline\n",
    "# Get average values for missing columns\n",
    "avg_values = last_known.select(\n",
    "    *[F.avg(c).alias(c) for c in last_known.columns if c not in [\"year\", \"region\", \"region_size_category\"]]\n",
    ").collect()[0].asDict()\n",
    "\n",
    "# Join future data with last known data\n",
    "future_data = future_data.join(\n",
    "    last_known.drop(\"year\"),  # Drop year to avoid conflict\n",
    "    \"region\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Update temporal features for future years\n",
    "future_data = (future_data\n",
    "    .withColumn(\"years_since_2008\", F.col(\"year\") - 2008)\n",
    "    .withColumn(\"is_covid_period\", F.lit(0))\n",
    "    .withColumn(\"growth_y\", F.lit(0.02))  # Assume 2% growth\n",
    ")\n",
    "\n",
    "# Fill nulls with average values for any missing regions\n",
    "for col_name in future_data.columns:\n",
    "    if col_name not in [\"year\", \"region\", \"region_size_category\"] and col_name in avg_values:\n",
    "        future_data = future_data.fillna({col_name: avg_values[col_name]})\n",
    "\n",
    "# Ensure all numeric columns are filled\n",
    "numeric_cols = [f.name for f in future_data.schema.fields \n",
    "                if f.dataType in [DoubleType(), IntegerType()] and f.name != \"year\"]\n",
    "for col in numeric_cols:\n",
    "    future_data = future_data.fillna({col: 0.0})\n",
    "\n",
    "# Add log_total if needed for GBT model\n",
    "future_data = future_data.withColumn(\n",
    "    \"log_total\", \n",
    "    F.when(F.col(TOTAL_COL) > 0, F.log1p(F.col(TOTAL_COL))).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Prepare features using the pipeline\n",
    "try:\n",
    "    future_prepared = feature_model.transform(future_data)\n",
    "    \n",
    "    # Use best model for predictions\n",
    "    if \"Forest\" in best_model_name:\n",
    "        future_predictions = rf_model.transform(future_prepared)\n",
    "        pred_col = \"prediction\"\n",
    "    elif \"GBT\" in best_model_name:\n",
    "        future_predictions = gbt_model.transform(future_prepared)\n",
    "        future_predictions = future_predictions.withColumn(\n",
    "            \"prediction_final\", \n",
    "            F.expm1(F.col(\"prediction\"))\n",
    "        )\n",
    "        pred_col = \"prediction_final\"\n",
    "    else:\n",
    "        future_predictions = glm_model.transform(future_prepared)\n",
    "        pred_col = \"prediction\"\n",
    "    \n",
    "    # Select and save future predictions\n",
    "    future_results = future_predictions.select(\n",
    "        \"year\", \n",
    "        \"region\", \n",
    "        F.col(pred_col).alias(\"predicted_accidents\")\n",
    "    ).orderBy(\"year\", \"region\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FUTURE PREDICTIONS (2024-2025) - Sample\")\n",
    "    print(\"=\"*50)\n",
    "    future_results.filter(F.col(\"region\").isin([\"LIMA\", \"AREQUIPA\", \"CUSCO\", \"PIURA\"])).show()\n",
    "    \n",
    "    # Save future predictions\n",
    "    (future_results\n",
    "     .coalesce(1)\n",
    "     .write.mode(\"overwrite\")\n",
    "     .option(\"header\", True)\n",
    "     .csv(str(DIR_MODELS / \"future_predictions_2024_2025\")))\n",
    "    \n",
    "    print(f\"\\nFuture predictions saved to: {DIR_MODELS / 'future_predictions_2024_2025'}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nWarning: Could not generate future predictions due to: {str(e)}\")\n",
    "    print(\"This may be due to missing historical data for 2023.\")\n",
    "    print(\"Attempting alternative approach using 2022 data...\")\n",
    "    \n",
    "    # Alternative: Use 2022 data if 2023 is not available\n",
    "    last_year_available = df_features.agg(F.max(\"year\")).collect()[0][0]\n",
    "    print(f\"Using data from year: {last_year_available}\")\n",
    "    \n",
    "    # Get the last available year data\n",
    "    last_known_alt = df_features.filter(F.col(\"year\") == last_year_available)\n",
    "    \n",
    "    # Create a simpler prediction based on historical averages\n",
    "    simple_predictions = []\n",
    "    for region_row in regions:\n",
    "        region_name = region_row.region\n",
    "        region_data = last_known_alt.filter(F.col(\"region\") == region_name)\n",
    "        \n",
    "        if region_data.count() > 0:\n",
    "            last_value = region_data.select(TOTAL_COL).collect()[0][0]\n",
    "            # Simple growth projection\n",
    "            for year in future_years:\n",
    "                growth_factor = 1.02 ** (year - last_year_available)\n",
    "                predicted_value = last_value * growth_factor\n",
    "                simple_predictions.append((year, region_name, predicted_value))\n",
    "    \n",
    "    if simple_predictions:\n",
    "        future_results = spark.createDataFrame(\n",
    "            simple_predictions,\n",
    "            [\"year\", \"region\", \"predicted_accidents\"]\n",
    "        ).orderBy(\"year\", \"region\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SIMPLE FUTURE PROJECTIONS (2024-2025) - Sample\")\n",
    "        print(\"=\"*50)\n",
    "        future_results.filter(F.col(\"region\").isin([\"LIMA\", \"AREQUIPA\", \"CUSCO\", \"PIURA\"])).show()\n",
    "        \n",
    "        # Save predictions\n",
    "        (future_results\n",
    "         .coalesce(1)\n",
    "         .write.mode(\"overwrite\")\n",
    "         .option(\"header\", True)\n",
    "         .csv(str(DIR_MODELS / \"future_predictions_2024_2025_simple\")))\n",
    "        \n",
    "        print(f\"\\nSimple projections saved to: {DIR_MODELS / 'future_predictions_2024_2025_simple'}\")\n",
    "\n",
    "# Use best model for predictions\n",
    "if \"Forest\" in best_model_name:\n",
    "    future_predictions = rf_model.transform(future_prepared)\n",
    "    pred_col = \"prediction\"\n",
    "elif \"GBT\" in best_model_name:\n",
    "    future_data_log = future_data.withColumn(\"log_total\", F.log1p(F.col(\"last_total\")))\n",
    "    future_prepared_log = feature_model.transform(future_data_log)\n",
    "    future_predictions = gbt_model.transform(future_prepared)\n",
    "    future_predictions = future_predictions.withColumn(\n",
    "        \"prediction\", \n",
    "        F.expm1(F.col(\"prediction\"))\n",
    "    )\n",
    "    pred_col = \"prediction\"\n",
    "else:\n",
    "    future_predictions = glm_model.transform(future_prepared)\n",
    "    pred_col = \"prediction\"\n",
    "\n",
    "# Select and save future predictions\n",
    "future_results = future_predictions.select(\n",
    "    \"year\", \n",
    "    \"region\", \n",
    "    F.col(pred_col).alias(\"predicted_accidents\")\n",
    ").orderBy(\"year\", \"region\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FUTURE PREDICTIONS (2024-2025) - Sample\")\n",
    "print(\"=\"*50)\n",
    "future_results.filter(F.col(\"region\").isin([\"LIMA\", \"AREQUIPA\", \"CUSCO\", \"PIURA\"])).show()\n",
    "\n",
    "# Save future predictions\n",
    "(future_results\n",
    " .coalesce(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(str(DIR_MODELS / \"future_predictions_2024_2025\")))\n",
    "\n",
    "print(f\"\\nFuture predictions saved to: {DIR_MODELS / 'future_predictions_2024_2025'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f825f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 14: Time Series Cross-Validation ===\n",
      "\n",
      "Split 1:\n",
      "  Train: 2009-2017\n",
      "  Test: 2018-2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:50 WARN DAGScheduler: Broadcasting large task binary with size 1094.9 KiB\n",
      "25/10/05 23:23:50 WARN DAGScheduler: Broadcasting large task binary with size 1276.9 KiB\n",
      "25/10/05 23:23:50 WARN DAGScheduler: Broadcasting large task binary with size 1523.2 KiB\n",
      "25/10/05 23:23:51 WARN DAGScheduler: Broadcasting large task binary with size 1884.8 KiB\n",
      "25/10/05 23:23:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/10/05 23:23:51 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/10/05 23:23:51 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 2:\n",
      "  Train: 2009-2019\n",
      "  Test: 2020-2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:54 WARN DAGScheduler: Broadcasting large task binary with size 1098.2 KiB\n",
      "25/10/05 23:23:55 WARN DAGScheduler: Broadcasting large task binary with size 1282.0 KiB\n",
      "25/10/05 23:23:55 WARN DAGScheduler: Broadcasting large task binary with size 1518.7 KiB\n",
      "25/10/05 23:23:55 WARN DAGScheduler: Broadcasting large task binary with size 1846.1 KiB\n",
      "25/10/05 23:23:55 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/10/05 23:23:55 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/10/05 23:23:56 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 3:\n",
      "  Train: 2009-2021\n",
      "  Test: 2022-2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 23:23:59 WARN DAGScheduler: Broadcasting large task binary with size 1091.7 KiB\n",
      "25/10/05 23:23:59 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
      "25/10/05 23:23:59 WARN DAGScheduler: Broadcasting large task binary with size 1494.1 KiB\n",
      "25/10/05 23:23:59 WARN DAGScheduler: Broadcasting large task binary with size 1820.6 KiB\n",
      "25/10/05 23:23:59 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/10/05 23:23:59 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/10/05 23:24:00 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION RESULTS\n",
      "==================================================\n",
      "Split 1: RMSE=3773.20, MAE=1110.84\n",
      "Split 2: RMSE=7658.82, MAE=2250.86\n",
      "Split 3: RMSE=1446.04, MAE=529.52\n",
      "\n",
      "Average CV Performance: RMSE=4292.69, MAE=1297.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 14. TIME SERIES CROSS-VALIDATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 14: Time Series Cross-Validation ===\")\n",
    "\n",
    "def time_series_cv(df, n_splits=3):\n",
    "    \"\"\"Perform time series cross-validation\"\"\"\n",
    "    years = df.select(\"year\").distinct().orderBy(\"year\").collect()\n",
    "    year_list = [r.year for r in years]\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        # Define split points\n",
    "        test_size = 2  # Use 2 years for testing\n",
    "        split_point = len(year_list) - (n_splits - split) * test_size\n",
    "        \n",
    "        if split_point <= 5:  # Need at least 5 years for training\n",
    "            continue\n",
    "            \n",
    "        train_years = year_list[:split_point]\n",
    "        test_years = year_list[split_point:split_point + test_size]\n",
    "        \n",
    "        print(f\"\\nSplit {split + 1}:\")\n",
    "        print(f\"  Train: {min(train_years)}-{max(train_years)}\")\n",
    "        print(f\"  Test: {min(test_years)}-{max(test_years)}\")\n",
    "        \n",
    "        # Create train/test sets\n",
    "        train_cv = df.filter(F.col(\"year\").isin(train_years))\n",
    "        test_cv = df.filter(F.col(\"year\").isin(test_years))\n",
    "        \n",
    "        # Train model (using Random Forest as best performer)\n",
    "        rf_cv = RandomForestRegressor(\n",
    "            labelCol=TOTAL_COL,\n",
    "            featuresCol=\"features\",\n",
    "            numTrees=100,\n",
    "            maxDepth=10,\n",
    "            seed=42 + split\n",
    "        )\n",
    "        \n",
    "        # Prepare data\n",
    "        train_prep = feature_model.transform(train_cv)\n",
    "        test_prep = feature_model.transform(test_cv)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model_cv = rf_cv.fit(train_prep)\n",
    "        pred_cv = model_cv.transform(test_prep)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = RegressionEvaluator(\n",
    "            labelCol=TOTAL_COL,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"rmse\"\n",
    "        ).evaluate(pred_cv)\n",
    "        \n",
    "        mae = RegressionEvaluator(\n",
    "            labelCol=TOTAL_COL,\n",
    "            predictionCol=\"prediction\",\n",
    "            metricName=\"mae\"\n",
    "        ).evaluate(pred_cv)\n",
    "        \n",
    "        cv_results.append({\n",
    "            \"split\": split + 1,\n",
    "            \"train_years\": f\"{min(train_years)}-{max(train_years)}\",\n",
    "            \"test_years\": f\"{min(test_years)}-{max(test_years)}\",\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae\n",
    "        })\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = time_series_cv(df_model, n_splits=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for result in cv_results:\n",
    "    print(f\"Split {result['split']}: RMSE={result['rmse']:.2f}, MAE={result['mae']:.2f}\")\n",
    "\n",
    "avg_rmse = sum(r['rmse'] for r in cv_results) / len(cv_results)\n",
    "avg_mae = sum(r['mae'] for r in cv_results) / len(cv_results)\n",
    "print(f\"\\nAverage CV Performance: RMSE={avg_rmse:.2f}, MAE={avg_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2ac793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 15: Residual Analysis ===\n",
      "\n",
      "==================================================\n",
      "RESIDUAL ANALYSIS BY REGION (Top 10)\n",
      "==================================================\n",
      "+--------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|              region|      mean_residual|      std_residual|       min_residual|       max_residual|  mean_std_residual|\n",
      "+--------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|                LIMA|   7688.10554166666|1661.0966653422163|  6513.532825396818|  8862.678257936503|  40.82095355235738|\n",
      "|            AREQUIPA|-1939.8066309523806| 531.3210174763245|-2315.5073253968258|-1564.1059365079354|-22.998715909939122|\n",
      "|        HUANCAVELICA|-1637.9941051587298|2344.1525394008854|-3295.5602619047613| 19.572051587301587|-27.060377233299178|\n",
      "|TOTAL SINIESTROS ...|  1125.548928447257| 2140.121863914702|-387.74575409242243| 2638.8436109869363| 3.8726325685078042|\n",
      "|               PIURA|-1058.6960634920636| 966.5997726981748|-1742.1853174603184| -375.2068095238087|-14.614179765433242|\n",
      "|           CAJAMARCA| -336.2923154761902| 522.7803825486221| -705.9538690476184|  33.36923809523796|  -6.28525622913814|\n",
      "|         TOTAL ANUAL| -291.7392936508404|2040.0178456220772|-1734.2497460317827|  1150.771158730102| -1.000402998545483|\n",
      "|               JUNIN|-180.60428769841246|197.53113514814595|-320.27989285714284|-40.928682539682086|-2.8666939991951867|\n",
      "|              CALLAO| -171.5205827922082| 205.7757160476043|-317.02598701298666|-26.015178571429715| -2.981422697341019|\n",
      "|                 ICA| -94.66652777777745|165.70988491541655|-211.84111111111088| 22.508055555555984|-1.9334819143568558|\n",
      "+--------------------+-------------------+------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "==================================================\n",
      "RESIDUAL PATTERNS BY YEAR\n",
      "==================================================\n",
      "+----+------------------+------------------+-----------------+\n",
      "|year|     mean_residual|      std_residual|mean_abs_residual|\n",
      "+----+------------------+------------------+-----------------+\n",
      "|2022|-12.58019975400956|  1411.17224226796|518.9889835408468|\n",
      "|2023| 236.0842276361396|1978.9958785696508|726.1938992169811|\n",
      "+----+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 15. RESIDUAL ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 15: Residual Analysis ===\")\n",
    "\n",
    "# Analyze residuals from best model\n",
    "residuals = best_predictions.withColumn(\n",
    "    \"residual\", \n",
    "    F.col(TOTAL_COL) - F.col(\"prediction\")\n",
    ").withColumn(\n",
    "    \"standardized_residual\",\n",
    "    F.col(\"residual\") / F.sqrt(F.abs(F.col(\"prediction\")) + 1)\n",
    ")\n",
    "\n",
    "# Residual statistics by region\n",
    "residual_stats = residuals.groupBy(\"region\").agg(\n",
    "    F.mean(\"residual\").alias(\"mean_residual\"),\n",
    "    F.stddev(\"residual\").alias(\"std_residual\"),\n",
    "    F.min(\"residual\").alias(\"min_residual\"),\n",
    "    F.max(\"residual\").alias(\"max_residual\"),\n",
    "    F.mean(\"standardized_residual\").alias(\"mean_std_residual\")\n",
    ").orderBy(F.desc(F.abs(\"mean_residual\")))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESIDUAL ANALYSIS BY REGION (Top 10)\")\n",
    "print(\"=\"*50)\n",
    "residual_stats.show(10)\n",
    "\n",
    "# Check for temporal patterns in residuals\n",
    "temporal_residuals = residuals.groupBy(\"year\").agg(\n",
    "    F.mean(\"residual\").alias(\"mean_residual\"),\n",
    "    F.stddev(\"residual\").alias(\"std_residual\"),\n",
    "    F.mean(F.abs(\"residual\")).alias(\"mean_abs_residual\")\n",
    ").orderBy(\"year\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESIDUAL PATTERNS BY YEAR\")\n",
    "print(\"=\"*50)\n",
    "temporal_residuals.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1acbf7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 16: Final Quality Checks ===\n",
      "\n",
      "==================================================\n",
      "FINAL QUALITY METRICS\n",
      "==================================================\n",
      "Data Recovery:\n",
      "  - Records recovered: 51\n",
      "  - Total records now: 447\n",
      "\n",
      "Prediction Accuracy:\n",
      "  - Within 20% error: 90.7%\n",
      "  - Within 50% error: 98.1%\n",
      "\n",
      "Lima Handling:\n",
      "  - Actual 2022: 41111\n",
      "  - Predicted 2022: 34597\n",
      "  - Error: 6514 (15.8%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 16. FINAL QUALITY CHECKS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 16: Final Quality Checks ===\")\n",
    "\n",
    "quality_checks = {\n",
    "    \"data_recovery\": {\n",
    "        \"regions_before_fix\": 396,  # From original notebook\n",
    "        \"regions_after_fix\": df_features.count(),\n",
    "        \"recovered_regions\": df_features.count() - 396\n",
    "    },\n",
    "    \"prediction_quality\": {\n",
    "        \"predictions_within_20pct\": best_predictions.filter(\n",
    "            F.col(\"relative_error\") <= 0.2\n",
    "        ).count(),\n",
    "        \"predictions_within_50pct\": best_predictions.filter(\n",
    "            F.col(\"relative_error\") <= 0.5\n",
    "        ).count(),\n",
    "        \"total_predictions\": best_predictions.count()\n",
    "    },\n",
    "    \"outlier_handling\": {\n",
    "        \"lima_actual_2022\": best_predictions.filter(\n",
    "            (F.col(\"region\") == \"LIMA\") & (F.col(\"year\") == 2022)\n",
    "        ).select(TOTAL_COL).collect()[0][0] if best_predictions.filter(\n",
    "            (F.col(\"region\") == \"LIMA\") & (F.col(\"year\") == 2022)\n",
    "        ).count() > 0 else None,\n",
    "        \"lima_predicted_2022\": best_predictions.filter(\n",
    "            (F.col(\"region\") == \"LIMA\") & (F.col(\"year\") == 2022)\n",
    "        ).select(\"prediction\").collect()[0][0] if best_predictions.filter(\n",
    "            (F.col(\"region\") == \"LIMA\") & (F.col(\"year\") == 2022)\n",
    "        ).count() > 0 else None\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL QUALITY METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Data Recovery:\")\n",
    "print(f\"  - Records recovered: {quality_checks['data_recovery']['recovered_regions']}\")\n",
    "print(f\"  - Total records now: {quality_checks['data_recovery']['regions_after_fix']}\")\n",
    "print(f\"\\nPrediction Accuracy:\")\n",
    "if quality_checks['prediction_quality']['total_predictions'] > 0:\n",
    "    pct_20 = (quality_checks['prediction_quality']['predictions_within_20pct'] / \n",
    "              quality_checks['prediction_quality']['total_predictions'] * 100)\n",
    "    pct_50 = (quality_checks['prediction_quality']['predictions_within_50pct'] / \n",
    "              quality_checks['prediction_quality']['total_predictions'] * 100)\n",
    "    print(f\"  - Within 20% error: {pct_20:.1f}%\")\n",
    "    print(f\"  - Within 50% error: {pct_50:.1f}%\")\n",
    "print(f\"\\nLima Handling:\")\n",
    "if quality_checks['outlier_handling']['lima_actual_2022']:\n",
    "    print(f\"  - Actual 2022: {quality_checks['outlier_handling']['lima_actual_2022']:.0f}\")\n",
    "    print(f\"  - Predicted 2022: {quality_checks['outlier_handling']['lima_predicted_2022']:.0f}\")\n",
    "    lima_error = abs(quality_checks['outlier_handling']['lima_actual_2022'] - \n",
    "                     quality_checks['outlier_handling']['lima_predicted_2022'])\n",
    "    lima_pct_error = lima_error / quality_checks['outlier_handling']['lima_actual_2022'] * 100\n",
    "    print(f\"  - Error: {lima_error:.0f} ({lima_pct_error:.1f}%)\")\n",
    "\n",
    "# Save quality checks\n",
    "with open(DIR_MODELS / \"quality_checks.json\", \"w\") as f:\n",
    "    # Convert to serializable format\n",
    "    serializable_checks = {}\n",
    "    for key, value in quality_checks.items():\n",
    "        if isinstance(value, dict):\n",
    "            serializable_checks[key] = {\n",
    "                k: float(v) if v is not None else None \n",
    "                for k, v in value.items()\n",
    "            }\n",
    "        else:\n",
    "            serializable_checks[key] = value\n",
    "    json.dump(serializable_checks, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b8fc61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 17: Generating Final Report ===\n",
      "\n",
      "======================================================================\n",
      "ENHANCED ML PIPELINE FOR TRAFFIC ACCIDENTS PREDICTION - PERU\n",
      "======================================================================\n",
      "\n",
      "EXECUTIVE SUMMARY\n",
      "-----------------\n",
      "This enhanced pipeline successfully addresses the data quality issues\n",
      "and improves prediction accuracy for traffic accidents across Peru.\n",
      "\n",
      "KEY IMPROVEMENTS:\n",
      "1. Data Recovery: Recovered lost regions by recalculating totals\n",
      "2. Feature Engineering: Added temporal, regional, and COVID indicators  \n",
      "3. Model Diversity: Tested multiple algorithms (GLM, RF, GBT)\n",
      "4. Lima Handling: Separate treatment for extreme outlier region\n",
      "5. Validation: Implemented time-series cross-validation\n",
      "\n",
      "PERFORMANCE METRICS:\n",
      "--------------------\n",
      "Best Model: Random_Forest\n",
      "- RMSE: 1694.83\n",
      "- MAE: 622.59\n",
      "- Cross-Validation Average RMSE: 4292.69\n",
      "\n",
      "DATA QUALITY:\n",
      "-------------\n",
      "- Total Regions Processed: 28\n",
      "- Years Covered: 2007 - 2023\n",
      "- Records in Final Dataset: 405\n",
      "- Train Size: 351\n",
      "- Test Size: 54\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "----------------\n",
      "1. Continue monitoring Lima separately due to its outlier nature\n",
      "2. Update model quarterly with new data\n",
      "3. Consider external factors (economic indicators, weather patterns)\n",
      "4. Implement alert system for unusual prediction deviations\n",
      "5. Collect more granular data (monthly/weekly) for better accuracy\n",
      "\n",
      "FILES GENERATED:\n",
      "----------------\n",
      "- gold_local/siniestros_features_enhanced.parquet\n",
      "- models/predictions_best_model/\n",
      "- models/future_predictions_2024_2025/\n",
      "- models/validation_report.json\n",
      "- models/quality_checks.json\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 17. GENERATE FINAL REPORT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== STEP 17: Generating Final Report ===\")\n",
    "\n",
    "final_report = f\"\"\"\n",
    "{'='*70}\n",
    "ENHANCED ML PIPELINE FOR TRAFFIC ACCIDENTS PREDICTION - PERU\n",
    "{'='*70}\n",
    "\n",
    "EXECUTIVE SUMMARY\n",
    "-----------------\n",
    "This enhanced pipeline successfully addresses the data quality issues\n",
    "and improves prediction accuracy for traffic accidents across Peru.\n",
    "\n",
    "KEY IMPROVEMENTS:\n",
    "1. Data Recovery: Recovered lost regions by recalculating totals\n",
    "2. Feature Engineering: Added temporal, regional, and COVID indicators  \n",
    "3. Model Diversity: Tested multiple algorithms (GLM, RF, GBT)\n",
    "4. Lima Handling: Separate treatment for extreme outlier region\n",
    "5. Validation: Implemented time-series cross-validation\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "--------------------\n",
    "Best Model: {best_model_name}\n",
    "- RMSE: {results[best_model_name]['RMSE']:.2f}\n",
    "- MAE: {results[best_model_name]['MAE']:.2f}\n",
    "- Cross-Validation Average RMSE: {avg_rmse:.2f}\n",
    "\n",
    "DATA QUALITY:\n",
    "-------------\n",
    "- Total Regions Processed: {df_features.select('region').distinct().count()}\n",
    "- Years Covered: {df_features.select(F.min('year')).collect()[0][0]} - {df_features.select(F.max('year')).collect()[0][0]}\n",
    "- Records in Final Dataset: {df_model.count()}\n",
    "- Train Size: {train_data.count()}\n",
    "- Test Size: {test_data.count()}\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "----------------\n",
    "1. Continue monitoring Lima separately due to its outlier nature\n",
    "2. Update model quarterly with new data\n",
    "3. Consider external factors (economic indicators, weather patterns)\n",
    "4. Implement alert system for unusual prediction deviations\n",
    "5. Collect more granular data (monthly/weekly) for better accuracy\n",
    "\n",
    "FILES GENERATED:\n",
    "----------------\n",
    "- {DIR_GOLD}/siniestros_features_enhanced.parquet\n",
    "- {DIR_MODELS}/predictions_best_model/\n",
    "- {DIR_MODELS}/future_predictions_2024_2025/\n",
    "- {DIR_MODELS}/validation_report.json\n",
    "- {DIR_MODELS}/quality_checks.json\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(final_report)\n",
    "\n",
    "# Save final report\n",
    "with open(DIR_MODELS / \"final_report.txt\", \"w\") as f:\n",
    "    f.write(final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d373aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pipeline Complete ===\n",
      "All outputs saved to:\n",
      "  - Data: gold_local\n",
      "  - Models: models\n",
      "\n",
      " Enhanced ML Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# CLEANUP\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Pipeline Complete ===\")\n",
    "print(f\"All outputs saved to:\")\n",
    "print(f\"  - Data: {DIR_GOLD}\")\n",
    "print(f\"  - Models: {DIR_MODELS}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"\\n Enhanced ML Pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
